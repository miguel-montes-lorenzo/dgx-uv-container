# services:
#   ${SERVICE}:
#     container_name: ${USER}-session
#     build:
#       context: .
#       args:
#         UID: ${HOST_UID}
#         GID: ${HOST_GID}
#         USERNAME: guest
#     networks: [alumnos]

#     volumes:
#       - /home/${USER}/workdata/:/mnt/workdata

#     environment:
#       - UV_CACHE_DIR=/home/guest/.cache/uv
#       - NVIDIA_VISIBLE_DEVICES=all
#       - NVIDIA_DRIVER_CAPABILITIES=compute,utility

#     gpus: all

#     user: "root"
#     command: |
#       bash -lc "
#       set -euo pipefail

#       mkdir -p /mnt/workdata/data /mnt/workdata/${UV_CACHE_DIR} /mnt/workdata/ssh
#       mkdir -p /home/guest /home/guest/.cache

#       ln -sfn /mnt/workdata/data /home/guest/data
#       ln -sfn /mnt/workdata/${UV_CACHE_DIR} /home/guest/.cache/uv

#       # Make ~/.ssh persistent: point it directly at the volume path
#       rm -rf /home/guest/.ssh
#       ln -sfn /mnt/workdata/ssh /home/guest/.ssh

#       chown -R ${HOST_UID}:${HOST_GID} /mnt/workdata || true

#       # Permissions for SSH keys (apply to the real directory in the volume)
#       chown -R ${HOST_UID}:${HOST_GID} /mnt/workdata/ssh /home/guest/data /home/guest/.cache
#       chmod 700 /mnt/workdata/ssh
#       chmod 600 /mnt/workdata/ssh/* 2>/dev/null || true

#       sudo -H -u guest bash -lc '
#         set -euo pipefail
#         export UV_CACHE_DIR=/home/guest/.cache/uv
#         python_interpreters_to_install=(3.12 3.13 3.14)
#         for v in \"$$\{python_interpreters_to_install[@]\}\"; do
#           uv --no-progress python install \"$$v\"
#         done
#       '

#       exec su -s /bin/bash guest -c \"sleep infinity\"
#       "

# networks:
#   alumnos:
#     external: true





# services:
#   ubuntu:
#     container_name: ${USER}-session
#     build:
#       context: .
#       args:
#         UID: ${HOST_UID}
#         GID: ${HOST_GID}
#         USERNAME: ${CONTAINER_USER}
#     networks: [alumnos]

#     volumes:
#       - ${HOST_VOLUME_DIR}:/mnt/workdata

#     environment:
#       - UV_CACHE_DIR=/home/${CONTAINER_USER}/.cache/uv
#       - NVIDIA_VISIBLE_DEVICES=all
#       - NVIDIA_DRIVER_CAPABILITIES=compute,utility
#       - DISPLAY_README_AT_STARTUP=true

#     gpus: all

#     user: "root"
#     command: |
#       bash -lc "
#       set -euo pipefail

#       mkdir -p /mnt/workdata/data /mnt/workdata/${UV_CACHE_DIR} /mnt/workdata/ssh
#       mkdir -p /home/${CONTAINER_USER} /home/${CONTAINER_USER}/.cache

#       ln -sfn /mnt/workdata/data /home/${CONTAINER_USER}/data
#       ln -sfn /mnt/workdata/${UV_CACHE_DIR} /home/${CONTAINER_USER}/.cache/uv

#       # Make ~/.ssh persistent: point it directly at the volume path
#       rm -rf /home/${CONTAINER_USER}/.ssh
#       ln -sfn /mnt/workdata/ssh /home/${CONTAINER_USER}/.ssh

#       chown -R ${HOST_UID}:${HOST_GID} /mnt/workdata || true

#       # Permissions for SSH keys (apply to the real directory in the volume)
#       chown -R ${HOST_UID}:${HOST_GID} /mnt/workdata/ssh /home/${CONTAINER_USER}/data /home/${CONTAINER_USER}/.cache
#       chmod 700 /mnt/workdata/ssh
#       chmod 600 /mnt/workdata/ssh/* 2>/dev/null || true

#       sudo -H -u ${CONTAINER_USER} bash -lc '
#         set -euo pipefail
#         export UV_CACHE_DIR=/home/${CONTAINER_USER}/.cache/uv
#         python_interpreters_to_install=(3.12 3.13 3.14)
#         for v in \"$$\{python_interpreters_to_install[@]\}\"; do
#           uv --no-progress python install \"$$v\"
#         done
#       '

#       exec su -s /bin/bash ${CONTAINER_USER} -c \"sleep infinity\"
#       "

# networks:
#   alumnos:
#     external: true







services:
  ubuntu:
    container_name: ${USER}-session
    build:
      context: .
      args:
        UID: ${HOST_UID}
        GID: ${HOST_GID}
        USERNAME: ${CONTAINER_USER}
    networks: [alumnos]

    volumes:
      - ${HOST_VOLUME_PATH}:/mnt/workdata

    environment:
      # - UV_CACHE_DIR=/home/${CONTAINER_USER}/.cache/uv
      - UV_CACHE_DIR=/mnt/workdata/${UV_CACHE_DIR}
      - UV_LINK_MODE=hardlink
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - DISPLAY_README_AT_STARTUP=true

    user: "root"

    command: |
      bash -lc "
      set -euo pipefail

      mkdir -p /mnt/workdata/data /mnt/workdata/${UV_CACHE_DIR} /mnt/workdata/uv_share /mnt/workdata/ssh

      mkdir -p /home/${CONTAINER_USER}/.cache /home/${CONTAINER_USER}/.local/share

      ln -sfn /mnt/workdata/data /home/${CONTAINER_USER}/data

      # IMPORTANT: force cache symlink to point to the bind mount
      rm -rf /home/${CONTAINER_USER}/.cache/uv
      ln -sfn /mnt/workdata/${UV_CACHE_DIR} /home/${CONTAINER_USER}/.cache/uv

      # IMPORTANT: persist uv interpreters/state
      rm -rf /home/${CONTAINER_USER}/.local/share/uv
      ln -sfn /mnt/workdata/uv_share /home/${CONTAINER_USER}/.local/share/uv

      rm -rf /home/${CONTAINER_USER}/.ssh
      ln -sfn /mnt/workdata/ssh /home/${CONTAINER_USER}/.ssh

      chown -R ${HOST_UID}:${HOST_GID} /mnt/workdata || true
      chown -R ${HOST_UID}:${HOST_GID} /home/${CONTAINER_USER} || true

      chmod 700 /mnt/workdata/ssh
      chmod 600 /mnt/workdata/ssh/* 2>/dev/null || true

      sudo -H -u ${CONTAINER_USER} bash -lc '
        set -euo pipefail

        # use the REAL cache dir on the mount, no ambiguity
        export UV_CACHE_DIR=/mnt/workdata/${UV_CACHE_DIR}

        python_interpreters_to_install=(3.12 3.13 3.14)
        for v in \"$$\{python_interpreters_to_install[@]\}\"; do
          uv --no-progress python install \"$$v\"
        done
      '

      exec su -s /bin/bash ${CONTAINER_USER} -c \"sleep infinity\"
      "

networks:
  alumnos:
    external: true
